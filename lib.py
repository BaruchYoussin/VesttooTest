# ARIMA(0,1,1) module
import numpy as np
import torch.nn


class Arima_0_1_1(torch.nn.Module):
    """ARIMA(0,1,1) time series.

    Represent the given data, a one time block, as a part of an unlimited ARIMA(0,1,1) time series,
    and learn the optimal ARIMA(0,1,1) parameters.
    The innovation terms are assumed to be normally distributed with zero mean.
    There is no transformation of data, and for this reason forward(..) is not implemented.
    """
    def __init__(self, arma_const=None, ma_coeff=None, std_innovation=None):
        super().__init__()
        self.arma_const = torch.nn.Parameter(torch.randn(1, dtype=torch.float) if arma_const is None
                                             else torch.tensor(arma_const, dtype=torch.float))
        self.ma_coeff = torch.nn.Parameter(0.5 * torch.randn(1, dtype=torch.float) if ma_coeff is None
                                           else torch.tensor(ma_coeff, dtype=torch.float))
        self.std_innovation = torch.nn.Parameter(1 + torch.randn(1, dtype=torch.float) if std_innovation is None
                                                 else torch.tensor(std_innovation, dtype=torch.float))
        # If self.std_innovation is negative, its absolute value is used as the std of the generated innovations.
        # The initial value Y_0 is not included since it is not used in learning.


def _ma_matrix_torch(ma_coeff: torch.Tensor, data_length: int) -> torch.Tensor:
    """Returns the matrix of the moving average transformation from innovations to their moving averages.

    :param ma_coeff: The moving average coefficient of MA(1).
    :param data_length: The length of the data block to be generated by moving average model.
    :returns (data_length, data_length + 1) tensor with self.ma_coeff on the main diagonal
        and 1 on the diagonal above it.  See the doc.
    """
    return ma_coeff * torch.tensor(
        np.eye(data_length, M=data_length + 1, k=0, dtype=np.float32)) + torch.tensor(
        np.eye(data_length, M=data_length + 1, k=1, dtype=np.float32))


def lstsq(A: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """Substitute to torch.linalg.lstsq which fails in Autograd.

    :param A: The matrix of the linear system.
    :param b: The right-hand sides of the linear system.  Must be of the appropriate size.
    :returns The least squares solution x of the linear system Ax = b.
    The failure has been reported as a bug in Torch 1.10:
    https://gitanswer.com/pytorch-lstsq-not-working-with-autograd-cplusplus-1022752878
    """
    return A.pinverse() @ b


def _solve_for_ma_innovations(model: Arima_0_1_1, data: torch.Tensor) -> torch.Tensor:
    """Solves for innovations producing a given a block of data by a specified MA(1) model.

    :param model: ARIMA(0, 1, 1) model that specifies the MA(1) model.
    :param data: 1-dim tensor.
    :returns The tensor of innovations, 1-dim of length = length (data) = 1, see the doc, normalized to std = 1.
    The innovation values for the given parameter values are not unique, and we choose the least squares
    of the normalized values; it is the maximal likelihood choice.
    """
    size = data.size()
    assert len(size) == 1
    length = size[0]
    solution = lstsq(_ma_matrix_torch(model.ma_coeff, length), data - model.arma_const)
    # solution, _, _, _ = torch.linalg.lstsq(_ma_matrix_torch(model.ma_coeff, length), data - model.arma_const,
    # driver="gels")
    return solution


def solve_for_innovations(model: Arima_0_1_1, time_block: torch.Tensor) -> torch.Tensor:
    """Find the best (the least squares) normalized innovations that produce a given contiguous time block of data.

    :param model: The model used to produce the data.
    :param time_block: a 1-dim object convertible to a 1-dim tensor.
    :returns The tensor of innovations normalized to std = 1.
    """
    return _solve_for_ma_innovations(model, torch.diff(time_block)) / model.std_innovation.abs()


def _ma_matrix_numpy(ma_coeff: float, data_length: int) -> np.array:
    """Returns the matrix of the moving average transformation from innovations to their moving averages.

    :param ma_coeff: the coefficient of the MA(1) model.
    :param data_length: The length of the data block to be generated by moving average model.
    :returns (data_length, data_length + 1) array with ma_coeff on the main diagonal
        and 1 on the diagonal above it.  See the doc.
    """
    return ma_coeff * np.eye(data_length, M=data_length + 1, k=0) + np.eye(data_length, M=data_length + 1, k=1)


def generate_arima_0_1_1(length: int, arma_const: float, ma_coeff: float, std_innovation:float,
                         initial_value:float, seed=None) -> np.array:
    innovations = np.random.default_rng(seed).normal(loc=0, scale= std_innovation, size=length)
    moving_averages = np.matmul(_ma_matrix_numpy(ma_coeff, length - 1), innovations) + arma_const
    return np.cumsum(np.concatenate(([initial_value], moving_averages)))


def old_loss(model: Arima_0_1_1, time_block: torch.Tensor) -> torch.Tensor:
    """Sum of the squares of the innovations."""
    if not isinstance(time_block, torch.Tensor):
        time_block = torch.tensor(time_block, dtype=torch.float)
    differences = time_block.diff()
    innovations = solve_for_innovations(model, differences)
    return (innovations * innovations).sum()


def _cov_matrix(ma_coeff: torch.Tensor, std_innovation: torch.Tensor, data_length: int) -> torch.Tensor:
    """Returns the covariance matrix per the doc, MaxLikelihoodARIMA.odt

    :param ma_coeff: theta_1 in the doc.
    :param std_innovation: sigma in the doc.
    :param data_length: the length of the difference series, n + 1 in the doc
    :returns The covariance matrix, data_length x data_length.
    """
    matrix = (ma_coeff ** 2 + 1) * torch.tensor(np.eye(data_length, k=0, dtype=np.float32)) + ma_coeff * torch.tensor(
        np.eye(data_length, k=1, dtype=np.float32) + np.eye(data_length, k=-1, dtype=np.float32))
    return std_innovation ** 2 * matrix


def _squares(X: torch.Tensor, arma_const: torch.Tensor, cov_matrix: torch.Tensor) -> torch.Tensor:
    """Returns (X - c) Cov^(-1) (X - c)^T in the doc, MaxLikelihoodARIMA.odt

    :param X: the time series of the differences, X in the doc.
    :param arma_const: c in the doc.
    :param cov_matrix: Cov in the doc.
    :returns as above.
    """
    matrix_inverse = cov_matrix.inverse()
    vector = X - arma_const
    return vector @ matrix_inverse @ vector.T


def loss(model: Arima_0_1_1, time_block) -> torch.Tensor:
    """Per the doc, MaxLikelihoodARIMA.odt

    :param model: The model to be learned.
    :param time_block: the original time series, X in the doc.
    :returns the loss as in the doc.
    """
    if not isinstance(time_block, torch.Tensor):
        time_block = torch.tensor(time_block, dtype=torch.float)
    differences = time_block.diff()
    sizeofX = differences.size()
    assert len(sizeofX) == 1
    length = sizeofX[0]
    cov_matrix = _cov_matrix(model.ma_coeff, model.std_innovation, length)
    return _squares(X=differences, arma_const=model.arma_const, cov_matrix=cov_matrix) + cov_matrix.logdet()
